---
phase: 04-history-archive
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/database/schema.js
  - server/routes/history.js
  - server/index.js
autonomous: true

must_haves:
  truths:
    - "Completed tasks older than 2 years are automatically deleted daily"
    - "History API returns completed tasks with filtering by date, employee, system, location"
    - "Statistics show total completed, late count, on-time percentage"
  artifacts:
    - path: "server/routes/history.js"
      provides: "History endpoint with multi-filter support and statistics"
      exports: ["router"]
      min_lines: 100
    - path: "server/services/dataRetention.js"
      provides: "Scheduled cleanup job with transaction safety"
      exports: ["initializeDataRetention", "cleanupOldTasks"]
      min_lines: 80
    - path: "server/database/schema.js"
      provides: "Composite indexes for history query performance"
      contains: "idx_tasks_history"
  key_links:
    - from: "server/routes/history.js"
      to: "tasks table"
      via: "composite index idx_tasks_history"
      pattern: "status.*completed_at.*employee_id.*system_id"
    - from: "server/services/dataRetention.js"
      to: "tasks table"
      via: "scheduled cron job"
      pattern: "cron\\.schedule.*0 2 \\* \\* \\*"
    - from: "server/index.js"
      to: "dataRetention service"
      via: "initialization call"
      pattern: "initializeDataRetention"
---

<objective>
Provide backend infrastructure for history viewing, filtering, statistics, and automated data retention with optimized query performance.

Purpose: Enable managers to access 2-year historical task data with multi-dimensional filtering and automated cleanup to maintain database size.

Output: History REST API with statistics, scheduled cleanup job, and performance-optimized database indexes.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-history-archive/04-RESEARCH.md

# Database and enrichment patterns
@server/database/schema.js
@server/routes/tasks.js

# Phase 3 timing infrastructure
@.planning/phases/03-status-tracking-timing/03-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create History API Endpoint with Multi-Filter Support</name>
  <files>server/routes/history.js</files>
  <action>
Create GET /api/history endpoint accepting query parameters:
- startDate, endDate (ISO date strings, defaults to last 7 days if not provided)
- employeeId, systemId, locationId (optional filter integers)
- limit (default 50), offset (default 0) for pagination

Implementation requirements:
- Build dynamic WHERE clause using parameterized queries (never string concatenation)
- Join systems, employees, locations tables to get names
- Filter by status = 'completed' always
- Apply optional filters only if query param exists
- Default to last 7 days if no date range provided: `completed_at >= datetime("now", "-7 days")`
- Sort by completed_at DESC (most recent first)
- Return 3 data structures in response:
  1. tasks array (filtered, enriched with system_name, employee_name, location_name)
  2. pagination object (total count, limit, offset, hasMore boolean)
  3. stats object (total_completed, total_late, on_time_percentage)

Statistics calculation (single query with CASE aggregation):
```sql
SELECT
  COUNT(*) as total_completed,
  SUM(CASE WHEN time_delta_minutes > 0 THEN 1 ELSE 0 END) as total_late,
  ROUND(100.0 * SUM(CASE WHEN time_delta_minutes <= 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as on_time_percentage
FROM tasks ...
WHERE [same conditions as main query]
```

Note decimal 100.0 (not 100) to prevent integer division returning 0.

Pattern: Same parameterized query building as RESEARCH.md Pattern 1 (Multi-Filter Query).

Mount router in server/index.js:
```javascript
const historyRoutes = require('./routes/history');
app.use('/api/history', historyRoutes);
```

Verify with: `curl http://localhost:3002/api/history?startDate=2025-01-01&endDate=2025-01-31` returns tasks, pagination, stats.
  </action>
  <verify>
Test endpoint with curl/Postman:
- GET /api/history (no filters) - returns last 7 days with stats
- GET /api/history?employeeId=1 - filters by employee
- GET /api/history?startDate=2025-01-01&endDate=2025-01-31 - filters by date range
- Response includes: tasks[], pagination object, stats object
- Stats show correct on_time_percentage as decimal (e.g., 88.2 not 0)
  </verify>
  <done>
History endpoint exists at /api/history, accepts all filter parameters, returns filtered tasks with statistics, pagination works correctly, percentage calculation uses decimal division.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Database Indexes for History Query Performance</name>
  <files>server/database/schema.js</files>
  <action>
Add composite index creation in initializeDatabase() function after task_attachments table creation:

```javascript
// History query optimization indexes
db.exec(`
  CREATE INDEX IF NOT EXISTS idx_tasks_history
  ON tasks(status, completed_at DESC, employee_id, system_id)
`);

db.exec(`
  CREATE INDEX IF NOT EXISTS idx_tasks_retention
  ON tasks(status, completed_at)
`);

// Enable WAL mode for better concurrency (reads during writes)
db.pragma('journal_mode = WAL');

// Increase cache size for better performance
db.pragma('cache_size = 50000');
```

Composite index column order rationale (per RESEARCH.md):
1. status (constant filter - always 'completed')
2. completed_at DESC (most selective variable - date range)
3. employee_id, system_id (optional filters)

Verify with EXPLAIN QUERY PLAN:
```javascript
const plan = db.prepare(`
  EXPLAIN QUERY PLAN
  SELECT * FROM tasks
  WHERE status = 'completed'
    AND completed_at >= datetime('now', '-7 days')
    AND employee_id = 1
`).all();
console.log('Query plan:', plan);
```

Look for "USING INDEX idx_tasks_history" in output (not "SCAN TABLE tasks").

Location filtering note: If systems table doesn't have location_id column, add migration:
```javascript
try {
  db.exec(`ALTER TABLE systems ADD COLUMN location_id INTEGER REFERENCES locations(id)`);
} catch (e) {
  // Column exists, ignore
}
```

Then history queries can JOIN through systems to filter by location.
  </action>
  <verify>
Run server and check console output:
- "History indexes created successfully" or similar message
- EXPLAIN QUERY PLAN output shows "USING INDEX idx_tasks_history"
- WAL mode enabled
- Query /api/history with 100+ tasks - response time < 100ms
  </verify>
  <done>
Composite indexes exist (idx_tasks_history, idx_tasks_retention), WAL mode enabled, EXPLAIN QUERY PLAN confirms index usage, history queries perform well with large datasets.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement Scheduled Data Retention with node-cron</name>
  <files>server/services/dataRetention.js, server/index.js, server/package.json</files>
  <action>
Install node-cron (per RESEARCH.md standard stack):
```bash
npm install node-cron
```

Create server/services/dataRetention.js with two exported functions:

1. **cleanupOldTasks()** - Manual cleanup function:
   - Count tasks to be deleted (for logging): `SELECT COUNT(*) FROM tasks WHERE status = 'completed' AND completed_at < datetime('now', '-2 years')`
   - If count is 0, log and return early
   - Delete in transaction using db.transaction():
     ```javascript
     const deleteStmt = db.prepare(`
       DELETE FROM tasks
       WHERE status = 'completed'
         AND completed_at < datetime('now', '-2 years')
     `);
     const result = db.transaction(() => deleteStmt.run())();
     ```
   - Log: start time, count before, deleted count, duration, any errors
   - All logs prefixed with `[Data Retention]` for grep-ability

2. **initializeDataRetention()** - Scheduler initialization:
   - Validate cron expression with cron.validate('0 2 * * *')
   - Schedule daily at 2:00 AM Israel time:
     ```javascript
     cron.schedule('0 2 * * *', cleanupOldTasks, {
       timezone: "Asia/Jerusalem",
       scheduled: true
     });
     ```
   - Log: "Cron job initialized (daily at 2:00 AM Israel time)"
   - Return { cleanupOldTasks } for manual testing if needed

In server/index.js, after database initialization:
```javascript
const { initializeDataRetention } = require('./services/dataRetention');
initializeDataRetention();
```

Pattern: Follow RESEARCH.md Pattern 4 (Scheduled Cleanup) with transaction safety.

Why transaction: Ensures all-or-nothing deletion. If cleanup crashes mid-execution, no partial deletion.

Verify manual trigger works:
```javascript
// In Node REPL or test file
const { cleanupOldTasks } = require('./services/dataRetention');
cleanupOldTasks(); // Should log count and result
```
  </action>
  <verify>
Test scheduled job initialization:
- Server starts without errors
- Console shows: "[Data Retention] Cron job initialized (daily at 2:00 AM Israel time)"
- Manual test: Create task with completed_at 3 years ago, run cleanupOldTasks(), task deleted
- Manual test: Create task with completed_at 1 year ago, run cleanupOldTasks(), task NOT deleted
- Transaction safety: No partial deletions on error
  </verify>
  <done>
node-cron installed, dataRetention service exists with cleanupOldTasks and initializeDataRetention functions, cron job scheduled for 2 AM daily, logs show job initialized, manual cleanup works correctly with 2-year retention, transaction ensures atomic deletion.
  </done>
</task>

</tasks>

<verification>
Backend history infrastructure complete when:
1. GET /api/history returns completed tasks with all filter parameters working
2. Statistics show correct percentages using decimal division
3. Pagination returns total count and hasMore flag
4. EXPLAIN QUERY PLAN shows index usage (not table scan)
5. Scheduled cleanup job initialized at server startup
6. Manual cleanup correctly deletes tasks >2 years old, preserves newer tasks
7. All operations use parameterized queries (no SQL injection risk)
</verification>

<success_criteria>
- Manager can fetch history via /api/history with date range filters
- Filtering by employee, system, location returns correct subset
- Statistics display: "102 משימות הושלמו, 12 באיחור (88.2% בזמן)"
- Query performance: 100+ tasks return in < 100ms
- Cleanup job logs show scheduled execution
- Tasks older than 2 years are deleted automatically
- Database size remains manageable with retention policy
</success_criteria>

<output>
After completion, create `.planning/phases/04-history-archive/04-01-SUMMARY.md` documenting:
- History endpoint contract (query params, response structure)
- Index performance characteristics
- Cleanup job schedule and transaction safety
- Any schema changes (location_id in systems table)
</output>
